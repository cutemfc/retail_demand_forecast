{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu4qgnk/byOHGt5JcmA1M9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cutemfc/retail_demand_forecast/blob/main/%E3%80%8CWeek4_streamlit_ipynb%E3%80%8DR1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Streamlit in Colab"
      ],
      "metadata": {
        "id": "WhRZnJ-rzwZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDKGxIKGzrMZ",
        "outputId": "bf98837c-4616-4eac-d9e7-2a976f100522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Packages streamlit and cloudfare"
      ],
      "metadata": {
        "id": "JxIMZv6gz5Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install cloudflared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA9PpaKsz4J0",
        "outputId": "d63a4eb7-76d3-4392-ff86-da3bf5fec78c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudflared\n",
            "  Downloading cloudflared-1.0.0.2.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setuptools_scm (from cloudflared)\n",
            "  Downloading setuptools_scm-8.3.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools_scm->cloudflared) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools_scm->cloudflared) (75.2.0)\n",
            "Downloading setuptools_scm-8.3.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cloudflared\n",
            "  Building wheel for cloudflared (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cloudflared: filename=cloudflared-1.0.0.2-py3-none-any.whl size=2983 sha256=c159575d4e9d6037ed7e0e1b5916d8f2d0ffb789ee436c6da15ec4ea9dbe9eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/9f/f1/ef5e36c9386d737ac05ab8714d611c430d79abe55d862ca2b7\n",
            "Successfully built cloudflared\n",
            "Installing collected packages: setuptools_scm, cloudflared\n",
            "Successfully installed cloudflared-1.0.0.2 setuptools_scm-8.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Add the cloudfare files"
      ],
      "metadata": {
        "id": "fQw4D-v30DtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "0ObdC0210AbL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Add entire Streamlit code below"
      ],
      "metadata": {
        "id": "4yEsXz5I0K0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data/config.py\n",
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/data/config.py\n",
        "DATA_PATH = \"/content/drive/MyDrive/retail_kaggle_data/data/\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/retail_kaggle_data/models/xgboost_model_revised.pkl\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn2vbsO00Lr1",
        "outputId": "0d846c28-72b2-4941-d733-9df7efa735a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/data/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data/utils\n",
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/data/data_utils.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "def load_data(DATA_PATH):\n",
        "    df_store = pd.read_csv(os.path.join(DATA_PATH, 'stores.csv'))\n",
        "    df_item = pd.read_csv(os.path.join(DATA_PATH, 'items.csv'))\n",
        "    df_train = pd.read_csv(os.path.join(DATA_PATH, 'df_train_revised.csv')) # Only selected stores in Guyas, Top 3 Families\n",
        "    return df_store, df_item, df_train\n",
        "\n",
        "def prepare_training_data(df_train):\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_train.set_index('date', inplace=True)\n",
        "    # Lag features\n",
        "    df_train['lag_1'] = df_train['unit_sales'].shift(1)\n",
        "    df_train['lag_7'] = df_train['unit_sales'].shift(7)\n",
        "    df_train['lag_30'] = df_train['unit_sales'].shift(30)\n",
        "    # Rolling features\n",
        "    df_train['rolling_mean_7'] = df_train['unit_sales'].rolling(window=7).mean()\n",
        "    df_train['rolling_std_7'] = df_train['unit_sales'].rolling(window=7).std()\n",
        "    df_train.dropna(inplace=True)\n",
        "    # Time-based features\n",
        "    df_train['year'] = df_train.index.year\n",
        "    df_train['month'] = df_train.index.month\n",
        "    df_train['day_of_week'] = df_train.index.dayofweek\n",
        "    df_train['is_weekend'] = df_train['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    # Outlier detection and replacement\n",
        "    z_scores = zscore(df_train['unit_sales'])\n",
        "    outliers = df_train[z_scores > 5]\n",
        "    df_train.loc[outliers.index, 'unit_sales'] = df_train.loc[outliers.index, 'rolling_mean_7']\n",
        "    return df_train.reset_index()  # Restore 'date' as a column\n",
        "\n",
        "def create_input_row(store_id, item_id, date, df_train):\n",
        "    # Based on the input item/store\n",
        "    date = pd.to_datetime(date)\n",
        "    relevant_data = df_train[\n",
        "        (df_train['store_nbr'] == store_id) & (df_train['item_nbr'] == item_id) & (df_train['date'] < date)\n",
        "    ]\n",
        "\n",
        "    if relevant_data.empty:\n",
        "        raise ValueError(\"No historical data for this item/store combination.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #lag features\n",
        "    lag_1 = relevant_data.iloc[-1]['unit_sales']\n",
        "    lag_7 = relevant_data.iloc[-7]['unit_sales']if len(relevant_data) >= 7 else lag_1\n",
        "    lag_30 = relevant_data.iloc[-30]['unit_sales'] if len(relevant_data) >= 30 else lag_7\n",
        "    rolling_data=relevant_data.iloc[-7:]['unit_sales']\n",
        "    rolling_mean_7 = relevant_data.iloc[-7:]['unit_sales'].mean() if len(rolling_data) == 7 else lag_1\n",
        "    rolling_std_7 = relevant_data.iloc[-7:]['unit_sales'].std() if len(rolling_data) == 7 else 0.0\n",
        "    latest_row = relevant_data.iloc[-1].copy()\n",
        "    input_data = latest_row.copy()\n",
        "\n",
        "    input_data['lag_1'] = lag_1\n",
        "    input_data['lag_7'] = lag_7\n",
        "    input_data['lag_30'] = lag_30\n",
        "    input_data['rolling_mean_7'] = rolling_mean_7\n",
        "    input_data['rolling_std_7'] = rolling_std_7\n",
        "\n",
        "    input_data['date'] = date\n",
        "    input_data['year'] = date.year\n",
        "    input_data['month'] = date.month\n",
        "    input_data['day_of_week'] = date.dayofweek\n",
        "    input_data['is_weekend'] = 1 if date.dayofweek >= 5 else 0\n",
        "    input_features = input_data[['store_nbr', 'item_nbr', 'id', 'onpromotion',\n",
        "                                 'lag_1', 'lag_7', 'lag_30',\n",
        "                                 'rolling_mean_7', 'rolling_std_7',\n",
        "                                 'year', 'month', 'day_of_week', 'is_weekend']]\n",
        "\n",
        "    st.write(\"\\n=== Input features  ===\")\n",
        "    st.write(input_features)\n",
        "    return input_features.astype(float)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRbVmjXb0VMS",
        "outputId": "e91c5e13-2f2d-49e9-834c-ad902172f928"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/data/data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/models/model_utils.py\n",
        "import pickle\n",
        "\n",
        "def load_model(MODEL_PATH):\n",
        "    with open(MODEL_PATH, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    return model\n",
        "\n",
        "def predict(model, input_df):\n",
        "    return model.predict(input_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EOAfbin2g9G",
        "outputId": "1582ae61-4dfb-4891-98fc-7102ac722688"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/models/model_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/utils.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def run_visualization(df_train, model, split_date='2014-01-01', max_plots=1, store_filter=None, item_filter=None):\n",
        "    split_date = pd.to_datetime(split_date)  # Ensure datetime\n",
        "    rmad_values = []\n",
        "    bias_values = []\n",
        "    rmse_values = []\n",
        "    mape_values = []\n",
        "    r2_values = []\n",
        "    plot_count = 0\n",
        "\n",
        "    # Check for required inputs\n",
        "    if store_filter is None or item_filter is None:\n",
        "        st.warning(\"Please provide both store and item filter.\")\n",
        "        return\n",
        "\n",
        "    # Filter the target group\n",
        "    target_group = df_train[(df_train['store_nbr'] == store_filter) & (df_train['item_nbr'] == item_filter)]\n",
        "    if target_group.empty:\n",
        "        st.warning(\"No data available for the selected item/store combination.\")\n",
        "        return\n",
        "\n",
        "    target_group = target_group.reset_index()\n",
        "    target_group['date'] = pd.to_datetime(target_group['date'])\n",
        "\n",
        "    test_series = target_group[target_group['date'] >= split_date]\n",
        "    train_series = target_group[target_group['date'] < split_date]\n",
        "\n",
        "    if len(test_series) <= 5:\n",
        "        st.warning(\"Not enough test data for visualization.\")\n",
        "        return\n",
        "\n",
        "    X_test = test_series.drop(['unit_sales', 'date'], axis=1)\n",
        "    y_test = test_series['unit_sales']\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if plot_count < max_plots:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(train_series['date'], train_series['unit_sales'], label='Train Sales', color='black')\n",
        "        plt.plot(test_series['date'], y_test, label='Actual Sales', color='blue')\n",
        "        plt.plot(test_series['date'], y_pred, label='Predicted Sales', color='red')\n",
        "        plt.title(f'Store {store_filter}, Item {item_filter}', fontsize=16)\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Unit Sales')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend()\n",
        "        st.pyplot(plt.gcf())\n",
        "        plt.close()\n",
        "        plot_count += 1\n",
        "\n",
        "    # Calculate the metrics\n",
        "    bias = np.mean(y_pred - y_test)\n",
        "    rmad = np.mean(np.abs(y_pred - y_test))\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Show the data\n",
        "    st.markdown(f\"### Store {store_filter}, Item {item_filter}\")\n",
        "    st.write(\n",
        "        f\"**Bias:** {bias:.2f} &nbsp;&nbsp; \"\n",
        "        f\"**RMAD:** {rmad:.2f} &nbsp;&nbsp; \"\n",
        "        f\"**RMSE:** {rmse:.2f} &nbsp;&nbsp; \"\n",
        "        f\"**MAPE:** {mape:.2f}% &nbsp;&nbsp; \"\n",
        "        f\"**R²:** {r2:.2f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYx8p_1vWZX7",
        "outputId": "57bb2e0c-fb79-407b-9898-5d6d6d4d27ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/app.py\n",
        "import sys\n",
        "import os\n",
        "import streamlit as st\n",
        "from data.config import DATA_PATH, MODEL_PATH\n",
        "from data.data_utils import load_data, prepare_training_data, create_input_row\n",
        "from models.model_utils import load_model, predict\n",
        "import datetime  # Used for handling date inputs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from utils import run_visualization\n",
        "\n",
        "\n",
        "@st.cache_resource  # Only read the data once, and repeat to use it\n",
        "def cache_load_data():\n",
        "    df_store, df_item, df_train = load_data(DATA_PATH)\n",
        "    df_train_processed = prepare_training_data(df_train)\n",
        "    return df_store, df_item, df_train, df_train_processed\n",
        "@st.cache_resource\n",
        "def cache_load_model():\n",
        "    return load_model(MODEL_PATH)\n",
        "\n",
        "def run_app():\n",
        "    st.title(\"Corporación Favorita Sales Forecasting\")\n",
        "    # load data and model from cache\n",
        "    df_store, df_item, df_train, df_train_processed = cache_load_data()\n",
        "    model=cache_load_model()\n",
        "\n",
        "    # Store selection\n",
        "    store_id = st.selectbox(\"Store\", [24, 26, 27, 28])  # For testing limit to one store\n",
        "    item_id = st.selectbox(\"Item\", [564533, 838216, 582865, 364606])  # For testing limit to a few items\n",
        "\n",
        "\n",
        "    # Set default and allowed date range for forecasting\n",
        "    default_date = datetime.date(2014, 1, 1)  # Default date is Jan 1, 2014\n",
        "    min_date = datetime.date(2013, 1, 2)  # Minimum date allowed is January 2, 2013\n",
        "    max_date = datetime.date(2014, 4, 1)  # Maximum date allowed is April 1, 2014\n",
        "\n",
        "    # Date input for selecting forecast date, within the range [min_date, max_date]\n",
        "    date = st.date_input(\"Forecast Date\", value=default_date, min_value=min_date, max_value=max_date)\n",
        "\n",
        "    # When the user clicks the \"Get Forecast\" button\n",
        "    if st.button(\"Get Forecast\"):\n",
        "        hist_data = df_train_processed[\n",
        "        (df_train_processed['store_nbr'] == store_id) &\n",
        "        (df_train_processed['item_nbr'] == item_id) &\n",
        "        (df_train_processed['date'] < pd.to_datetime(date))\n",
        "        ]\n",
        "        st.write(f\"Number of historical records: {len(hist_data)}\")\n",
        "\n",
        "        if hist_data.empty:\n",
        "            st.warning(\"No historical data found for selection\")\n",
        "        else:\n",
        "            input_data = create_input_row(store_id, item_id, date, df_train_processed)\n",
        "            prediction = predict(model, input_data.to_frame().T)\n",
        "            st.write(f\"Predicted Sales for {date}: {prediction[0]}\")\n",
        "\n",
        "    # Visualization\n",
        "    split_date=st.sidebar.date_input(\"Forecast Start Date\",value=default_date,min_value=min_date, max_value=max_date)\n",
        "    run_visualization(df_train, model, split_date,store_filter=store_id, item_filter=item_id)\n",
        "\n",
        "# Ensure the script runs the main function if executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    run_app()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHG_XDOF3RCf",
        "outputId": "d27ca108-c59c-4601-908e-9b5e0ed2b21b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Run streamlit in the background"
      ],
      "metadata": {
        "id": "cmfPDe7k4Pv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/retail_kaggle_data/\n",
        "!streamlit run app.py &> logs.txt &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGMiXxT04MqJ",
        "outputId": "7b4b1300-1637-4a6e-f8b7-5eab1290a336"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/retail_kaggle_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create a Cloudfare Tunnel"
      ],
      "metadata": {
        "id": "kzuvejyU4di-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "h0i1k2g74eq5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cloudflared tunnel --url http://localhost:8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohf9Ej9k4js1",
        "outputId": "7ae01c07-ec9a-4efc-9182-0bf9a46bab4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[90m2025-05-19T16:51:31Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-05-19T16:51:31Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m |  https://asn-tables-asthma-older.trycloudflare.com                                         |\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.5.0 (Checksum a62266fd02041374f1fca0d85694aafdf7e26e171a314467356b471d4ebb2393)\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.22.10, GoArch: amd64\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 2a6cb4da-f5be-41b5-95d6-fb0ccbf588e5\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-05-19T16:51:35Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [CurveID(4588) CurveID(25497) CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "2025/05/19 16:51:35 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-05-19T16:51:36Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m1a1e5087-9557-40e7-b102-5df59bc27f47 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13 \u001b[36mlocation=\u001b[0miad05 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}