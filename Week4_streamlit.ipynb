{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxkgVGFzz5XJKL+zCnejje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cutemfc/retail_demand_forecast/blob/main/Week4_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Streamlit in Colab"
      ],
      "metadata": {
        "id": "WhRZnJ-rzwZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDKGxIKGzrMZ",
        "outputId": "d6184781-818d-46cd-e7b5-123e0d39ca28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Packages streamlit and cloudfare"
      ],
      "metadata": {
        "id": "JxIMZv6gz5Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install cloudflared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA9PpaKsz4J0",
        "outputId": "ccaf068c-02f4-4ea4-8042-0379423dbf2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cloudflared in /usr/local/lib/python3.11/dist-packages (1.0.0.2)\n",
            "Requirement already satisfied: setuptools-scm in /usr/local/lib/python3.11/dist-packages (from cloudflared) (8.3.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from setuptools-scm->cloudflared) (24.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from setuptools-scm->cloudflared) (75.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Add the cloudfare files"
      ],
      "metadata": {
        "id": "fQw4D-v30DtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "0ObdC0210AbL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Add entire Streamlit code below"
      ],
      "metadata": {
        "id": "4yEsXz5I0K0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data/config.py\n",
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/data/config.py\n",
        "DATA_PATH = \"/content/drive/MyDrive/retail_kaggle_data/data/\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/retail_kaggle_data/models/xgboost_model_revised.pkl\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn2vbsO00Lr1",
        "outputId": "7c82a8ef-c1f6-469c-8328-fc103878eb83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/data/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data/utils\n",
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/data/data_utils.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "def load_data(DATA_PATH):\n",
        "    df_store = pd.read_csv(os.path.join(DATA_PATH, 'stores.csv'))\n",
        "    df_item = pd.read_csv(os.path.join(DATA_PATH, 'items.csv'))\n",
        "    df_train = pd.read_csv(os.path.join(DATA_PATH, 'df_train_revised.csv')) # Only selected stores in Guyas, Top 3 Families\n",
        "    return df_store, df_item, df_train\n",
        "\n",
        "def prepare_training_data(df_train):\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_train.set_index('date', inplace=True)\n",
        "    # Lag features\n",
        "    df_train['lag_1'] = df_train['unit_sales'].shift(1)\n",
        "    df_train['lag_7'] = df_train['unit_sales'].shift(7)\n",
        "    df_train['lag_30'] = df_train['unit_sales'].shift(30)\n",
        "    # Rolling features\n",
        "    df_train['rolling_mean_7'] = df_train['unit_sales'].rolling(window=7).mean()\n",
        "    df_train['rolling_std_7'] = df_train['unit_sales'].rolling(window=7).std()\n",
        "    df_train.dropna(inplace=True)\n",
        "    # Time-based features\n",
        "    df_train['year'] = df_train.index.year\n",
        "    df_train['month'] = df_train.index.month\n",
        "    df_train['day_of_week'] = df_train.index.dayofweek\n",
        "    df_train['is_weekend'] = df_train['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "    # Outlier detection and replacement\n",
        "    z_scores = zscore(df_train['unit_sales'])\n",
        "    outliers = df_train[z_scores > 5]\n",
        "    df_train.loc[outliers.index, 'unit_sales'] = df_train.loc[outliers.index, 'rolling_mean_7']\n",
        "    return df_train.reset_index()  # Restore 'date' as a column\n",
        "\n",
        "def create_input_row(store_id, item_id, date, df_train):\n",
        "    # Based on the input item/store\n",
        "    date = pd.to_datetime(date)\n",
        "    relevant_data = df_train[\n",
        "        (df_train['store_nbr'] == store_id) & (df_train['item_nbr'] == item_id)\n",
        "    ]\n",
        "\n",
        "    if relevant_data.empty:\n",
        "        raise ValueError(\"No historical data for this item/store combination.\")\n",
        "\n",
        "    latest_row = relevant_data[relevant_data['date'] < date].sort_values(by='date').iloc[-1]\n",
        "\n",
        "    input_data = latest_row.copy()\n",
        "    input_data['date'] = date\n",
        "    input_data['year'] = date.year\n",
        "    input_data['month'] = date.month\n",
        "    input_data['day_of_week'] = date.dayofweek\n",
        "    input_data['is_weekend'] = 1 if date.dayofweek >= 5 else 0\n",
        "\n",
        "    return input_data.drop(['unit_sales', 'date'])  # X features only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRbVmjXb0VMS",
        "outputId": "3a5fc042-88ba-4fdb-ef54-b94437f8836a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/data/data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/models/model_utils.py\n",
        "import pickle\n",
        "\n",
        "def load_model(MODEL_PATH):\n",
        "    with open(MODEL_PATH, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    return model\n",
        "\n",
        "def predict(model, input_df):\n",
        "    return model.predict(input_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EOAfbin2g9G",
        "outputId": "0302765f-c9c4-49de-eec8-68dbbb585a27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/models/model_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create utils for data visualization and prediction evaluation\n",
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/utils.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def run_visualization(df_train, model, split_date='2014-01-01', max_plots=5):\n",
        "    split_date = pd.to_datetime(split_date)  # Ensure datetime\n",
        "    rmad_values = []\n",
        "    bias_values = []\n",
        "    rmse_values = []\n",
        "    mape_values = []\n",
        "    r2_values = []\n",
        "    plot_count = 0\n",
        "\n",
        "    for (item_nbr, store_nbr), group in df_train.groupby(['item_nbr', 'store_nbr']):\n",
        "        group = group.reset_index()\n",
        "        group['date'] = pd.to_datetime(group['date'])  # Ensure datetime\n",
        "        test_series = group[group['date'] >= split_date]\n",
        "\n",
        "        if len(test_series) > 5:\n",
        "            X_test = test_series.drop(['unit_sales', 'date'], axis=1)\n",
        "            y_test = test_series['unit_sales']\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            if plot_count < max_plots:\n",
        "                train_series = group[group['date'] < split_date]\n",
        "\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                plt.plot(train_series['date'], train_series['unit_sales'], label='Train Sales', color='black')\n",
        "                plt.plot(test_series['date'], y_test, label='Actual Sales', color='blue')\n",
        "                plt.plot(test_series['date'], y_pred, label='Predicted Sales', color='red')\n",
        "                plt.title(f'Store {store_nbr}, Item {item_nbr}', fontsize=16)\n",
        "                plt.xlabel('Date')\n",
        "                plt.ylabel('Unit Sales')\n",
        "                plt.xticks(rotation=45)\n",
        "                plt.legend()\n",
        "                st.pyplot(plt.gcf())\n",
        "                plt.close()\n",
        "                plot_count += 1\n",
        "            # Calculate the metrics\n",
        "            bias = np.mean(y_pred - y_test)\n",
        "            rmad = np.mean(np.abs(y_pred - y_test))\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            # Collect the data\n",
        "            bias_values.append(bias)\n",
        "            rmad_values.append(rmad)\n",
        "            rmse_values.append(rmse)\n",
        "            mape_values.append(mape)\n",
        "            r2_values.append(r2)\n",
        "\n",
        "            # Show the data\n",
        "            st.markdown(f\"### Store {store_nbr}, Item {item_nbr}\")\n",
        "            st.write(\n",
        "                f\"**Bias:** {bias:.2f} &nbsp;&nbsp; \"\n",
        "                f\"**RMAD:** {rmad:.2f} &nbsp;&nbsp; \"\n",
        "                f\"**RMSE:** {rmse:.2f} &nbsp;&nbsp; \"\n",
        "                f\"**MAPE:** {mape:.2f}% &nbsp;&nbsp; \"\n",
        "                f\"**R²:** {r2:.2f}\"\n",
        "            )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7P-JXq12iqu",
        "outputId": "a86e733e-87d6-4721-c2de-a5084f36f0be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/retail_kaggle_data/app.py\n",
        "import sys\n",
        "import os\n",
        "import streamlit as st\n",
        "from data.config import DATA_PATH, MODEL_PATH\n",
        "from data.data_utils import load_data, prepare_training_data, create_input_row\n",
        "from models.model_utils import load_model, predict\n",
        "import datetime  # Used for handling date inputs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from utils import run_visualization\n",
        "\n",
        "\n",
        "@st.cache_resource  # Only read the data once, and repeat to use it\n",
        "def cache_load_data():\n",
        "    df_store, df_item, df_train = load_data(DATA_PATH)\n",
        "    df_train_processed = prepare_training_data(df_train)\n",
        "    return df_store, df_item, df_train, df_train_processed\n",
        "@st.cache_resource\n",
        "def cache_load_model():\n",
        "    return load_model(MODEL_PATH)\n",
        "\n",
        "def run_app():\n",
        "    st.title(\"Corporación Favorita Sales Forecasting\")\n",
        "    # load data and model from cache\n",
        "    df_store, df_item, df_train, df_train_processed = cache_load_data()\n",
        "    model=cache_load_model()\n",
        "\n",
        "    # Store selection\n",
        "    store_id = st.selectbox(\"Store\", [1])  # For testing limit to one store\n",
        "    item_id = st.selectbox(\"Item\", [564533, 838216, 582865, 364606])  # For testing limit to a few items\n",
        "\n",
        "    # Set default and allowed date range for forecasting\n",
        "    default_date = datetime.date(2014, 1, 1)  # Default date is Jan 1, 2014\n",
        "    min_date = datetime.date(2013, 1, 1)  # Minimum date allowed is January 1, 2013\n",
        "    max_date = datetime.date(2014, 4, 1)  # Maximum date allowed is April 1, 2014\n",
        "\n",
        "    # Date input for selecting forecast date, within the range [min_date, max_date]\n",
        "    date = st.date_input(\"Forecast Date\", value=default_date, min_value=min_date, max_value=max_date)\n",
        "\n",
        "    # When the user clicks the \"Get Forecast\" button\n",
        "    if st.button(\"Get Forecast\"):\n",
        "        input_data = create_input_row(store_id, item_id, date, df_train_processed)\n",
        "        # Use the model to predict sales based on the input data\n",
        "        prediction = predict(model, input_data.to_frame().T)\n",
        "        # Display the predicted sales for the selected date\n",
        "        st.write(f\"Predicted Sales for {date}: {prediction[0]}\")\n",
        "    # Visualization\n",
        "    split_date=pd.to_datetime(date)\n",
        "    run_visualization(df_train, model, split_date)\n",
        "\n",
        "# Ensure the script runs the main function if executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    run_app()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHG_XDOF3RCf",
        "outputId": "8155014d-f6f4-4846-f85c-7a5637c7562f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/retail_kaggle_data/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Run streamlit in the background"
      ],
      "metadata": {
        "id": "cmfPDe7k4Pv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/retail_kaggle_data/\n",
        "!streamlit run app.py &> logs.txt &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGMiXxT04MqJ",
        "outputId": "85106298-5c00-4745-8d13-8647ed552968"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/retail_kaggle_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Create a Cloudfare Tunnel"
      ],
      "metadata": {
        "id": "kzuvejyU4di-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x cloudflared"
      ],
      "metadata": {
        "id": "h0i1k2g74eq5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cloudflared tunnel --url http://localhost:8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohf9Ej9k4js1",
        "outputId": "9578e0c1-2591-4b07-dfae-200b17ccf0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-05-18T21:14:57Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-05-18T21:14:57Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m |  https://strap-join-paul-lean.trycloudflare.com                                            |\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.5.0 (Checksum a62266fd02041374f1fca0d85694aafdf7e26e171a314467356b471d4ebb2393)\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.22.10, GoArch: amd64\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 377f4e58-90f6-4d65-a0bb-050722205e44\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [CurveID(4588) CurveID(25497) CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.107\n",
            "2025/05/18 21:15:01 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-05-18T21:15:01Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m892d9b3d-fa17-4300-8ba6-ebd5ef969289 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.107 \u001b[36mlocation=\u001b[0mlax09 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}